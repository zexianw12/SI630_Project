{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from rouge import Rouge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('max_colwidth',20)\n",
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset's size is  215365\n"
     ]
    }
   ],
   "source": [
    "wikihowAll_data = pd.read_csv(\"wikihowAll.csv\")\n",
    "print(\"The dataset's size is \", len(wikihowAll_data['headline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nKeep related s...</td>\n",
       "      <td>How to Be an Org...</td>\n",
       "      <td>If you're a pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nCreate a sketc...</td>\n",
       "      <td>How to Create a ...</td>\n",
       "      <td>See the image f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nGet a bachelor...</td>\n",
       "      <td>How to Be a Visu...</td>\n",
       "      <td>It is possible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nStart with som...</td>\n",
       "      <td>How to Become an...</td>\n",
       "      <td>The best art in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              headline                title                 text\n",
       "0  \\nKeep related s...  How to Be an Org...   If you're a pho...\n",
       "1  \\nCreate a sketc...  How to Create a ...   See the image f...\n",
       "2  \\nGet a bachelor...  How to Be a Visu...   It is possible ...\n",
       "3  \\nStart with som...  How to Become an...   The best art in..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikihowAll_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset's size is  185892\n"
     ]
    }
   ],
   "source": [
    "# remove all the NAN value in the dataset\n",
    "wikihowAll_data_clean = wikihowAll_data.dropna(subset=['headline', 'title', 'text'])\n",
    "wikihowAll_data_clean = wikihowAll_data_clean.reset_index(drop=True)\n",
    "# remove some short text in the dataset\n",
    "index = []\n",
    "for i in range(len(wikihowAll_data_clean['text'])):\n",
    "    if len(nltk.sent_tokenize(wikihowAll_data_clean['text'][i])) < 5:\n",
    "        index.append(i)\n",
    "wikihowAll_data_clean = wikihowAll_data_clean.drop(index)\n",
    "# remove the duplicate data\n",
    "wikihowAll_data_clean = wikihowAll_data_clean.drop_duplicates()\n",
    "wikihowAll_data_clean = wikihowAll_data_clean.reset_index(drop=True)\n",
    "print(\"The dataset's size is \", len(wikihowAll_data_clean['headline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = wikihowAll_data_clean.shape[0]\n",
    "\n",
    "train_rows = int(num_rows * 0.70)\n",
    "dev_rows = int(num_rows * 0.15)\n",
    "test_rows = num_rows - train_rows - dev_rows\n",
    "\n",
    "train_data = wikihowAll_data_clean[:train_rows]\n",
    "dev_data = wikihowAll_data_clean[train_rows:train_rows+dev_rows]\n",
    "test_data = wikihowAll_data_clean[train_rows+dev_rows:]\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "dev_data = dev_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27885"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lead-N baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lead_N_summary(text):\n",
    "    # Divide by paragraph\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    \n",
    "    # Get the first sentence of each paragraph\n",
    "    first_sentences = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "        if sentences:\n",
    "            first_sentences.append(sentences[0])\n",
    "            \n",
    "    # Join all of the sentence to be the summary\n",
    "    if len(first_sentences) == 0:\n",
    "        sentences_list = text.split(\"\\n\")\n",
    "        first_sentences = sentences_list[0:3]\n",
    "        \n",
    "    summary = \" \".join(first_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_N_summarization = []\n",
    "for text in test_data['text']:\n",
    "    lead_N_summarization.append(lead_N_summary(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27885"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_summarization = test_data['headline'].tolist()\n",
    "len(reference_summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.25997837365051885,\n",
       "  'p': 0.222851777015474,\n",
       "  'r': 0.42541135936903135},\n",
       " 'rouge-2': {'f': 0.06437508050099851,\n",
       "  'p': 0.05451236379524374,\n",
       "  'r': 0.10827056042030156},\n",
       " 'rouge-l': {'f': 0.22753192699108923,\n",
       "  'p': 0.18790073392745946,\n",
       "  'r': 0.36854726963371465}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "Lead_N_scores = rouge.get_scores(lead_N_summarization, reference_summarization, avg=True)\n",
    "Lead_N_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = \"glove.6B.50d.txt\"\n",
    "word_embeddings = {}\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        word_embed = np.asarray(values[1:]).astype(\"float\")\n",
    "        word_embeddings[word] = word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextRank_summary(text, num_sentence):\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    clean_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # remove punctuations and numbers\n",
    "        clean_sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        \n",
    "        # set all of the numbers to lower case\n",
    "        clean_sentence = clean_sentence.lower()\n",
    "        \n",
    "        # remove the stop words\n",
    "        clean_sentence_token = nltk.word_tokenize(clean_sentence)\n",
    "        clean_sentence_token = [word for word in clean_sentence_token if word not in stop_words]\n",
    "        clean_sentence = \" \".join(clean_sentence_token)\n",
    "        \n",
    "        clean_sentences.append(clean_sentence)\n",
    "        \n",
    "    # Get the sentence\n",
    "    sentence_vectors = []\n",
    "    for sentence in clean_sentences:\n",
    "        if len(sentence) != 0:\n",
    "            sentence_token = nltk.word_tokenize(sentence)\n",
    "            sentence_vector =  np.mean([word_embeddings.get(word, np.zeros((50,))) for word in sentence_token], axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.zeros((50,))\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "    \n",
    "    similarity_mat = np.zeros((len(clean_sentences), len(clean_sentences)))\n",
    "    \n",
    "    for i in range(len(sentence_vectors)):\n",
    "        for j in range(len(sentence_vectors)):\n",
    "            if i != j:\n",
    "                sentence_i = sentence_vectors[i].reshape((1, 50))\n",
    "                sentence_j = sentence_vectors[j].reshape((1, 50))\n",
    "                similarity_mat[i][j] = cosine_similarity(sentence_i, sentence_j)[0][0]\n",
    "    \n",
    "    sentence_graph = nx.from_numpy_array(similarity_mat)\n",
    "    sentence_scores = nx.pagerank(sentence_graph, max_iter=500, tol=1e-5, nstart={node: 1.0 for node in sentence_graph.nodes()})\n",
    "    \n",
    "    ranked_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        ranked_sentences.append((sentence_scores[i], sentence))\n",
    "    ranked_sentences = sorted(ranked_sentences, reverse=True)\n",
    "    \n",
    "    sentences = []\n",
    "    if len(ranked_sentences) < num_sentence:\n",
    "        num_sentence = len(ranked_sentences) - 1\n",
    "    for i in range(num_sentence):\n",
    "        sentences.append(ranked_sentences[i][1])\n",
    "    summary = \" \".join(sentences)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there might be some example can't be convergence, just skip it.\n",
    "TextRank_summarization = []\n",
    "num_sentence = 3\n",
    "for text in test_data['text']:\n",
    "    TextRank_summarization.append(TextRank_summary(text, num_sentence))\n",
    "len(TextRank_summarization)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.2370048915839522,\n",
       "  'p': 0.22073572879888995,\n",
       "  'r': 0.3434142197247943},\n",
       " 'rouge-2': {'f': 0.0551547797639333,\n",
       "  'p': 0.05056655469616582,\n",
       "  'r': 0.0831455867890583},\n",
       " 'rouge-l': {'f': 0.21716238793240872,\n",
       "  'p': 0.18952708684209132,\n",
       "  'r': 0.32194096409460754}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "TextRank_scores = rouge.get_scores(TextRank_summarization, reference_summarization, avg=True)\n",
    "TextRank_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si630",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad410ad0d6b71855dc6ddd8f8c9bc549304865da044ce610637a348f144ce9c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
